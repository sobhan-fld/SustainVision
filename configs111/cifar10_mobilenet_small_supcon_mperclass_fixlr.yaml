model: mobilenet_v3_small
database: databases/cifar10
device: cuda:1
seed: 42
optimizer: sgd
loss_function: supcon
weight_decay: 0.0001

scheduler:
  type: cosine_annealing
  params:
    eta_min: 0.0
    t_max_strategy: per_batch
    warmup_epochs: 10
  step_on_batch: true

gradient_clip_norm: 1.0
mixed_precision: true
save_model: true
save_model_path: outputs/mobilenet_small_supcon_mperclass_cifar10_fixlr
checkpoint_path: null
freeze_backbone: false

early_stopping:
  enabled: false
  patience: 3
  metric: val_loss
  mode: min

simclr_schedule:
  enabled: true
  # Multi-cycle: pretrain → evaluate (frozen backbone) → continue pretrain → evaluate...
  # Each cycle evaluates backbone quality with fresh linear head to track improvement
  # V3: Scheduler now continues across cycles for continued learning
  # fixlr: Learning rate now continues smoothly across cycles (no reset)
  cycles: 10
  pretrain_epochs: 100
  finetune_epochs: 40
  pretrain_loss: supcon
  finetune_loss: cross_entropy
  finetune_lr: 0.1
  finetune_optimizer: sgd
  finetune_weight_decay: 0.0001
  freeze_backbone: true
  optimizer_reset: true
  use_reference_transforms: true
  linear_subset_per_class: null
  linear_subset_seed: 42

report_filename: mobilenet_small_supcon_mperclass_cifar10_fixlr/mobilenet_small_supcon_mperclass_cifar10_fixlr.csv

hyperparameters:
  batch_size: 250  # Must match sampler output: 10 classes * 25 samples per class
  lr: 0.25
  epochs: 300  # NOTE: Ignored when simclr_schedule.enabled=true (uses pretrain_epochs per cycle instead)
  momentum: 0.9
  temperature: 0.25
  num_workers: 12
  val_split: 0.0
  image_size: 32
  projection_dim: 128
  projection_hidden_dim: 2048
  projection_use_bn: true
  use_gaussian_blur: true
  simclr_reference_transforms: true
  linear_subset_per_class: null
  linear_subset_seed: 42
  # M-per-class sampler for SupCon: ensures each batch has multiple samples per class
  # This guarantees positive pairs exist in every batch, crucial for SupCon learning
  use_m_per_class_sampler: true
  m_per_class: 25  # 10 classes * 25 samples = 250 batch size
