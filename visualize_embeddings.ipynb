{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# t-SNE Visualizer for SustainVision Checkpoints\n",
        "\n",
        "Use this notebook to project model embeddings to 2D with t-SNE. You can compare checkpoints trained with different objectives (e.g., cross-entropy vs SimCLR).\n",
        "\n",
        "**Workflow**\n",
        "1. Set the checkpoint path in the first cell below.\n",
        "2. Run all cells.\n",
        "3. Inspect the scatter plot to see how well classes separate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Configuration\n",
        "CHECKPOINT_PATH = \"resnet18_simclr_checkpoints/resnet18_simclr_model.pt\"  # path to saved checkpoint\n",
        "DATASET = \"cifar10\"\n",
        "NUM_SAMPLES = 2000\n",
        "DEVICE = \"cuda:1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sustainvision.config import TrainingConfig\n",
        "from sustainvision.data import build_classification_dataloaders\n",
        "from sustainvision.training import _build_model\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Device summary\n",
        "def describe_devices():\n",
        "    print(\"=== Device Summary ===\")\n",
        "    print(f\"Requested device: {DEVICE}\")\n",
        "    if torch.cuda.is_available():\n",
        "        count = torch.cuda.device_count()\n",
        "        print(f\"CUDA devices: {count}\")\n",
        "        for idx in range(count):\n",
        "            name = torch.cuda.get_device_name(idx)\n",
        "            props = torch.cuda.get_device_properties(idx)\n",
        "            total_gb = props.total_memory / (1024 ** 3)\n",
        "            with torch.cuda.device(idx):\n",
        "                free, total = torch.cuda.mem_get_info()\n",
        "                used_gb = (total - free) / (1024 ** 3)\n",
        "                print(f\"  GPU {idx}: {name} | used {used_gb:.2f} GB / {total_gb:.2f} GB\")\n",
        "    else:\n",
        "        print(\"CUDA not available; CPU only.\")\n",
        "\n",
        "describe_devices()\n",
        "\n",
        "device = torch.device(DEVICE if torch.cuda.is_available() and DEVICE.startswith(\"cuda\") else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Load checkpoint and dataset\n",
        "checkpoint_path = Path(CHECKPOINT_PATH)\n",
        "if not checkpoint_path.is_absolute():\n",
        "    checkpoint_path = Path.cwd() / checkpoint_path\n",
        "\n",
        "if not checkpoint_path.exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "\n",
        "print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "if \"config\" in checkpoint:\n",
        "    config = TrainingConfig(**checkpoint[\"config\"])\n",
        "else:\n",
        "    print(\"[warn] No config stored in checkpoint. Using defaults.\")\n",
        "    config = TrainingConfig()\n",
        "\n",
        "train_loader, _, num_classes = build_classification_dataloaders(\n",
        "    DATASET,\n",
        "    batch_size=128,\n",
        "    num_workers=2,\n",
        "    val_split=0.0,\n",
        "    seed=config.seed,\n",
        "    project_root=Path.cwd(),\n",
        "    image_size=config.hyperparameters.get(\"image_size\", 224),\n",
        "    contrastive=config.loss_function in {\"simclr\", \"supcon\"},\n",
        ")\n",
        "\n",
        "model = _build_model(\n",
        "    config.model,\n",
        "    num_classes=num_classes,\n",
        "    image_size=config.hyperparameters.get(\"image_size\", 224),\n",
        "    projection_dim=config.hyperparameters.get(\"projection_dim\", 128),\n",
        ").to(device)\n",
        "\n",
        "state_key = None\n",
        "for key_candidate in [\"model_state\", \"model\"]:\n",
        "    if key_candidate in checkpoint:\n",
        "        state_key = key_candidate\n",
        "        break\n",
        "\n",
        "if state_key:\n",
        "    model.load_state_dict(checkpoint[state_key], strict=False)\n",
        "else:\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "model.eval()\n",
        "print(f\"Model: {config.model} ({config.loss_function})\")\n",
        "print(f\"Classes: {num_classes}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Collect embeddings (limited by NUM_SAMPLES)\n",
        "target_samples = NUM_SAMPLES\n",
        "embeddings = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, batch_labels in tqdm(train_loader, desc=\"Embedding batches\"):\n",
        "        inputs = inputs.to(device)\n",
        "        if hasattr(model, \"backbone\"):\n",
        "            feats = model.backbone(inputs)\n",
        "            if isinstance(feats, tuple):\n",
        "                feats = feats[0]\n",
        "            if feats.ndim > 2:\n",
        "                feats = torch.flatten(feats, 1)\n",
        "        else:\n",
        "            feats, _ = model(inputs)\n",
        "        embeddings.append(feats.cpu())\n",
        "        labels.append(batch_labels)\n",
        "\n",
        "        if sum(len(x) for x in labels) >= target_samples:\n",
        "            break\n",
        "\n",
        "embeddings = torch.cat(embeddings, dim=0)[:target_samples]\n",
        "labels = torch.cat(labels, dim=0)[:target_samples]\n",
        "\n",
        "print(f\"Collected embeddings: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Run t-SNE\n",
        "emb_np = embeddings.numpy()\n",
        "print(\"Running t-SNE (this may take a while for large NUM_SAMPLES)...\")\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42, verbose=1)\n",
        "emb_2d = tsne.fit_transform(emb_np)\n",
        "print(\"t-SNE finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "palette = sns.color_palette(\"tab10\", n_colors=num_classes)\n",
        "sns.scatterplot(\n",
        "    x=emb_2d[:, 0],\n",
        "    y=emb_2d[:, 1],\n",
        "    hue=labels.numpy(),\n",
        "    palette=palette,\n",
        "    s=30,\n",
        "    alpha=0.7,\n",
        "    linewidth=0\n",
        ")\n",
        "plt.title(f\"t-SNE: {config.model} ({config.loss_function})\")\n",
        "plt.xlabel(\"t-SNE dimension 1\")\n",
        "plt.ylabel(\"t-SNE dimension 2\")\n",
        "plt.legend(title=\"Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Samples visualized: {len(emb_2d)}\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- Try different checkpoints (cross-entropy vs SimCLR) to compare clusters.\n",
        "- Adjust `NUM_SAMPLES` to trade off speed vs detail.\n",
        "- Experiment with `perplexity` in the t-SNE cell for different scales.\n",
        "- Change `DEVICE` to another GPU (e.g., `cuda:0`) or to `cpu` if needed."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
